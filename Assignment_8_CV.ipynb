{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using our own terms and diagrams, explain INCEPTIONNET ARCHITECTURE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> An Inception Network is a deep neural network that consists of repeating blocks where the output of a block act as an input to the next block. Each block is defined as an Inception block.Inception model is a convolutional neural network which helps in classifying the different types of objects on images. Also known as GoogLeNet. It uses ImageNet dataset for training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Describe the Inception block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans-->An Inception Module is an image model block that aims to approximate an optimal local sparse structure in a CNN. Put simply, it allows for us to use multiple types of filter size, instead of being restricted to a single filter size, in a single image block, which we then concatenate and pass onto the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What is the DIMENSIONALITY REDUCTION LAYER (1 LAYER CONVOLUTIONAL)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans-->A 1 x 1 Convolution is a convolution with some special properties in that it can be used for dimensionality reduction, efficient low dimensional embeddings, and applying non-linearity after convolutions. It maps an input pixel with all its channels to an output pixel which can be squeezed to a desired output depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. THE IMPACT OF REDUCING DIMENSIONALITY ON NETWORK PERFORMANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> Dimensionality reduction technique has been used for enhancement of performance of predictor. It has been seen that reduced features provide 15–18% more accuracy. Dimensionality reduction had many advantages including:\n",
    "Fewer features mean less complexity.\n",
    "You will need less storage space because you have fewer data.\n",
    "Fewer features require less computation time.\n",
    "Model accuracy improves due to less misleading data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Mention three components. Style GoogLeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans-->There are 22 Parameterized Layers in the Google Net architecture; these are Convolutional Layers and Fully-Connected Layers; if we include the non-parameterized layers like Max-Pooling, there are a total of 27 layers in the GoogleNet Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Using our own terms and diagrams, explain RESNET ARCHITECTURE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans-->ResNet is an artificial neural network that introduced a so-called “identity shortcut connection,” which allows the model to skip one or more layers. This approach makes it possible to train the network on thousands of layers without affecting performance. ResNets are one of the most efficient Neural Network Architectures, as they help in maintaining a low error rate much deeper in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. What do Skip Connections entail?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans-->Skip connections are a type of shortcut that connects the output of one layer to the input of another layer that is not adjacent to it. For example, in a CNN with four layers, A, B, C, and D, a skip connection could connect layer A to layer C, or layer B to layer D, or both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. What is the definition of a residual Block?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans-->A residual block is a stack of layers set in such a way that the output of a layer is taken and added to another layer deeper in the block. The non-linearity is then applied after adding it together with the output of the corresponding layer in the main path.There are two main types of ResNets blocks: The identity block and the convolutional block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. How can transfer learning help with problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans-->Transfer learning helps developers take a blended approach from different models to fine-tune a solution to a specific problem. The sharing of knowledge between two different models can result in a much more accurate and powerful model. The approach allows for the building models in an iterative way.It  is a method for reusing a model trained on a related predictive modeling problem. Transfer learning can be used to accelerate the training of neural networks as either a weight initialization scheme or feature extraction method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. What is transfer learning, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans-->Transfer learning is a machine learning method where we reuse a pre-trained model as the starting point for a model on a new task. To put it simply—a model trained on one task is repurposed on a second, related task as an optimization that allows rapid progress when modeling the second task.Three categories of transfer learning. Depending on the task and the amount of labeled/unlabeled data available for source and target domains, transfer learning methods fall into one of three categories: inductive, transductive, and unsupervised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.HOW DO NEURAL NETWORKS LEARN FEATURES? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans-->Artificial neural networks learn continuously by using corrective feedback loops to improve their predictive analytics. In simple terms,we can think of the data flowing from the input node to the output node through many different paths in the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. WHY IS FINE-TUNING BETTER THAN START-UP TRAINING?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans-->Fine-tuning can also adapt a pre-trained model to a new domain or improve its performance on a specific task. This technique can significantly reduce the time and computational resources required to train a deep-learning model from scratch.It allows us to take advantage of what the model has already learned without having to develop it from scratch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
